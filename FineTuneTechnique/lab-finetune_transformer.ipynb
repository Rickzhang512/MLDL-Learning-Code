{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4efadd59-b8c3-4503-96f9-551aa8ae04c8",
   "metadata": {},
   "source": [
    "# Fine-tuning a Pre-trained Transformer for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb7b30b-6bb4-4140-a43d-d47b4e6c9ddf",
   "metadata": {},
   "source": [
    "In this lab you will fine-tune a pre-trained transformer model using the huggingface `transformers` library. This library provides a number of transformer models such as BERT, XLNet, and GPT, that can be used with PyTorch or Tensorflow. The tokenisers for these models are also included, which makes using transformers with this library much easier than developing them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c79194-7436-4f62-b871-d05c70bc8b92",
   "metadata": {},
   "source": [
    "The [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) which is a smaller version of BERT will be used in this lab. It is much faster to fine-tune, but will give slightly worse performance than the original BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73e48ea-0371-4f75-a0c7-ce8558d5f63f",
   "metadata": {},
   "source": [
    "For the dataset we will use the [IMDB movie review data](https://huggingface.co/datasets/imdb) where the task is to classify a review as either positive if the reviewer liked the movie or negative if the reviewer did not like the movie. The input is the text of the review and the output is a binary label either 0 (negative) or 1 (positive).\n",
    "In previous labs and the assignments, you have explored this dataset multiple times but with a differnt train/validation/test splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192d17cb-dfcb-4934-aa04-5d00e30e59a0",
   "metadata": {},
   "source": [
    "We will use both `pytorch` and the `transformers` library, as well as a few other useful libraries such as `tqdm` to make progress bars, and `sklearn` for its evaluation metric.\n",
    "\n",
    "We will also use the `datasets` library which is the huggingface datasets library and can be installed using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "id": "8264b8ca-f0cf-4fe0-b356-ca24091bda87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:29:45.817048Z",
     "start_time": "2024-09-28T01:29:30.260739Z"
    }
   },
   "source": "!pip install transformers datasets ipywidgets",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (4.44.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (8.1.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (3.16.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\apex\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------- ----------- 41.0/57.6 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 41.0/57.6 kB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 430.6 kB/s eta 0:00:00\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.7-cp310-cp310-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipywidgets) (8.22.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipywidgets) (5.14.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.11 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipywidgets) (4.0.11)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.11 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipywidgets) (3.0.11)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.13.1-cp310-cp310-win_amd64.whl.metadata (52 kB)\n",
      "     ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 52.5/52.5 kB ? eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.10.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: executing in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "   ---------------------------------------- 0.0/471.6 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 112.6/471.6 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 358.4/471.6 kB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 358.4/471.6 kB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 358.4/471.6 kB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 471.6/471.6 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.10.7-cp310-cp310-win_amd64.whl (380 kB)\n",
      "   ---------------------------------------- 0.0/380.7 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 245.8/380.7 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 245.8/380.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 380.7/380.7 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading pyarrow-17.0.0-cp310-cp310-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/25.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.4/25.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.9/25.1 MB 10.9 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.9/25.1 MB 10.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.3/25.1 MB 6.5 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.3/25.1 MB 6.5 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.8/25.1 MB 6.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.8/25.1 MB 6.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.3/25.1 MB 5.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.3/25.1 MB 5.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 2.3/25.1 MB 5.9 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 5.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 5.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.0/25.1 MB 5.6 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 4.0/25.1 MB 5.6 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.7/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.7/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.3/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.3/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.6/25.1 MB 5.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.0/25.1 MB 5.9 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 6.0/25.1 MB 5.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.7/25.1 MB 6.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 5.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 7.5/25.1 MB 6.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.5/25.1 MB 6.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.3/25.1 MB 6.3 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.3/25.1 MB 6.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 8.8/25.1 MB 6.2 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.2/25.1 MB 6.3 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.5/25.1 MB 6.2 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/25.1 MB 6.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 10.0/25.1 MB 6.5 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.9/25.1 MB 6.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.9/25.1 MB 6.6 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 11.9/25.1 MB 7.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 11.9/25.1 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.9/25.1 MB 7.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 13.0/25.1 MB 7.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.0/25.1 MB 8.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.0/25.1 MB 8.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 15.0/25.1 MB 9.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.2/25.1 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.2/25.1 MB 9.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 16.3/25.1 MB 9.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.3/25.1 MB 9.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.6/25.1 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 17.6/25.1 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 18.9/25.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.0/25.1 MB 10.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 19.9/25.1 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.3/25.1 MB 11.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.9/25.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.7/25.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.7/25.1 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.2/25.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.8/25.1 MB 12.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.3/25.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.0/25.1 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.0/25.1 MB 12.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 10.7 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.4/78.4 kB ? eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB ? eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.4/50.4 kB ? eta 0:00:00\n",
      "Downloading multidict-6.1.0-cp310-cp310-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.13.1-cp310-cp310-win_amd64.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.4/111.4 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, tqdm, pyarrow, multidict, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.2\n",
      "    Uninstalling tqdm-4.66.2:\n",
      "      Successfully uninstalled tqdm-4.66.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed aiohappyeyeballs-2.4.2 aiohttp-3.10.7 aiosignal-1.3.1 async-timeout-4.0.3 datasets-3.0.1 dill-0.3.8 frozenlist-1.4.1 multidict-6.1.0 multiprocess-0.70.16 pyarrow-17.0.0 tqdm-4.66.5 xxhash-3.5.0 yarl-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "af6fa4ff-a38d-409f-8845-1b26f6e93de4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:30:50.097146Z",
     "start_time": "2024-09-28T01:30:50.092146Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "65ab68a8-0652-4cc5-96aa-b946404fdc76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:30:51.833210Z",
     "start_time": "2024-09-28T01:30:51.755582Z"
    }
   },
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "02357842-704f-4be0-a32f-e59e8187a295",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "First, load the `imdb` data using the huggingface `datasets` library, which will automatically download the data if it is not already cached on your system."
   ]
  },
  {
   "cell_type": "code",
   "id": "5e7b8631-e6fc-4ca6-932a-c6fd6adbd3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:31:17.882445Z",
     "start_time": "2024-09-28T01:30:55.018292Z"
    }
   },
   "source": [
    "data = load_dataset(\"imdb\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84c9729004f541edbd39f65724789ed8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\apex\\.cache\\huggingface\\hub\\datasets--imdb. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd019b98776f411f8c36c8ee1d539208"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cc81ddd1f01414f91616a46dc182244"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16fa3ab11bbb44d984a758ce0f5aea98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f855a938b90d492493257da9ae57a556"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d372c97770394c62a9cef5aeb85ece05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43afe53b9a9b4a4d811861904b66dbd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "a5642686-66a4-4372-83d6-880f9fefcd0f",
   "metadata": {},
   "source": [
    "If you print the `data` variable you will see that you have a dictionary like object with keys `test`, `train`, `unsupervised`, and values of `TorchIterableDataset`. We will only use the `test` and `train` data for this lab. You can inspect the first element of the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a88193e5-a0cc-43bc-a186-ff7b1875aede",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:31:21.723618Z",
     "start_time": "2024-09-28T01:31:21.715617Z"
    }
   },
   "source": [
    "pprint(data['train'][0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the '\n",
      "         'controversy that surrounded it when it was first released in 1967. I '\n",
      "         'also heard that at first it was seized by U.S. customs if it ever '\n",
      "         'tried to enter this country, therefore being a fan of films '\n",
      "         'considered \"controversial\" I really had to see this for myself.<br '\n",
      "         '/><br />The plot is centered around a young Swedish drama student '\n",
      "         'named Lena who wants to learn everything she can about life. In '\n",
      "         'particular she wants to focus her attentions to making some sort of '\n",
      "         'documentary on what the average Swede thought about certain '\n",
      "         'political issues such as the Vietnam War and race issues in the '\n",
      "         'United States. In between asking politicians and ordinary denizens '\n",
      "         'of Stockholm about their opinions on politics, she has sex with her '\n",
      "         'drama teacher, classmates, and married men.<br /><br />What kills me '\n",
      "         'about I AM CURIOUS-YELLOW is that 40 years ago, this was considered '\n",
      "         'pornographic. Really, the sex and nudity scenes are few and far '\n",
      "         \"between, even then it's not shot like some cheaply made porno. While \"\n",
      "         'my countrymen mind find it shocking, in reality sex and nudity are a '\n",
      "         'major staple in Swedish cinema. Even Ingmar Bergman, arguably their '\n",
      "         'answer to good old boy John Ford, had sex scenes in his films.<br '\n",
      "         '/><br />I do commend the filmmakers for the fact that any sex shown '\n",
      "         'in the film is shown for artistic purposes rather than just to shock '\n",
      "         'people and make money to be shown in pornographic theaters in '\n",
      "         'America. I AM CURIOUS-YELLOW is a good film for anyone wanting to '\n",
      "         'study the meat and potatoes (no pun intended) of Swedish cinema. But '\n",
      "         \"really, this film doesn't have much of a plot.\"}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "ffe4bb87-c4db-4707-b07f-06713cbf9d97",
   "metadata": {},
   "source": [
    "If you do this you will get a dictionary with both the `label` and the `text` of the first element. The label indicates if this text is a positive or negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a0ca8-bd5d-4fbe-af17-642dd8580666",
   "metadata": {},
   "source": [
    "### Tokeniser\n",
    "\n",
    "Since the `text` is currently a string of characters we need to:\n",
    "1. Split the sequence into tokens (i.e. word-pieces for BERT models but will be different depending on the pre-trained model)\n",
    "2. Represent the sequence as token/word-piece ids\n",
    "3. Add a `[CLS]` token to the start of the sequence, and a `[SEP]` token to the end\n",
    "4. Pad the sequences with `0`'s so they are all the same length\n",
    "5. Construct an attention mask for the input (to identify which parts of the input are padding and so should be ignored)\n",
    "\n",
    "Fortunately, the `transformers` library provides an easy way to do all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e6d3e-00fe-4e07-88ed-0dafcfaf3c46",
   "metadata": {},
   "source": [
    "First, get the tokeniser specific to DistilBert ([documentation here](https://huggingface.co/docs/transformers/model_doc/distilbert)). \\\n",
    "This will download the tokeniser for the lowercase only version of DistilBert. The download is only a few kilobytes. "
   ]
  },
  {
   "cell_type": "code",
   "id": "6751491b-b420-48fd-b44e-080f59ef25a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:31:36.666909Z",
     "start_time": "2024-09-28T01:31:33.652618Z"
    }
   },
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tok = DistilBertTokenizerFast.from_pretrained(model_name)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cff94c401db7472f9ef17eb72276d20e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\apex\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "03c049bdf9d040a28d182f673cfedf72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c860791b6d14e3b951429d0921f31f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce910c375de34abab882b8069d340dc9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apex\\anaconda3\\envs\\mldl\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "aed0e1df-14a7-464e-b766-7d1455feaad6",
   "metadata": {},
   "source": [
    "Next, write a function to apply the tokeniser to the `text` field in the dataset.\n",
    "\n",
    "This will do all the steps 1-5 listed above and return the attention mask and the sequence of ids. The arguments indicate we are truncating sequences longer than the maximum length, and padding all sequences that are less than the maximum length so that they are exactly the maximum length. The maximum length for the DistilBert model is 512 word-pieces."
   ]
  },
  {
   "cell_type": "code",
   "id": "8e46fed0-54e8-4997-90fd-aa9d22a0a984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:32:07.159951Z",
     "start_time": "2024-09-28T01:32:07.147952Z"
    }
   },
   "source": [
    "def tokenize_fn(X):\n",
    "    return tok(X[\"text\"], truncation = True, padding=\"max_length\")"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "f559facb-191c-4767-8daf-3726021f1512",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72026794-0265-4e92-a549-856242cff977",
   "metadata": {},
   "source": [
    "Now we can apply the tokeniser to the text as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "a80bd979-9f01-402c-a755-27718a6b1cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:32:20.143627Z",
     "start_time": "2024-09-28T01:32:13.892253Z"
    }
   },
   "source": [
    "shuffle_train_data = data['train'].shuffle(seed=42)\n",
    "tokenized_train_data = shuffle_train_data.map(tokenize_fn, batched=True)\n",
    "small_train_data = tokenized_train_data.select(range(1000))\n",
    "small_train_data = small_train_data.with_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba2b469fa24f409488789cc22e3cc4d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "5b5a9645-a2bf-4c65-b41a-93a1a541f762",
   "metadata": {},
   "source": [
    "The part which actually applies the tokeniser is the `map` call. This will call the `tokenize_fn` function on each of the examples in `data['train']`.\n",
    "\n",
    "The `shuffle` ensures the data is in a random order (if you don't do this you will run into problems because the dataset has all the 0 classes first followed by all the 1 classes).\n",
    "\n",
    "The `select` statement just extracts the first 1000 examples (of the shuffled list), if you were training this model to get the best performance you would use all the examples but for this lab we will use only 1000 to avoid waiting around for the model to train.\n",
    "\n",
    "The final line which calls `with_format` is responsible for converting the data columns `label`, `input_ids`, and `attention_mask` into PyTorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753957f-05eb-41b9-b7d4-f78cc55bfb0f",
   "metadata": {},
   "source": [
    "It is worthwhile to inspect `small_train_data` at this point. To do this use:"
   ]
  },
  {
   "cell_type": "code",
   "id": "2682d99d-3a1a-487e-a258-fef033e4595e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:34:20.064570Z",
     "start_time": "2024-09-28T01:34:20.050571Z"
    }
   },
   "source": "print(small_train_data[0])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
      " 'input_ids': tensor([  101,  2045,  2003,  2053,  7189,  2012,  2035,  2090,  3481,  3771,\n",
      "         1998,  6337,  2099,  2021,  1996,  2755,  2008,  2119,  2024,  2610,\n",
      "         2186,  2055,  6355,  6997,  1012,  6337,  2099,  3504, 15594,  2100,\n",
      "         1010,  3481,  3771,  3504,  4438,  1012,  6337,  2099, 14811,  2024,\n",
      "         3243,  3722,  1012,  3481,  3771,  1005,  1055,  5436,  2024,  2521,\n",
      "         2062,  8552,  1012,  1012,  1012,  3481,  3771,  3504,  2062,  2066,\n",
      "         3539,  8343,  1010,  2065,  2057,  2031,  2000,  3962, 12319,  1012,\n",
      "         1012,  1012,  1996,  2364,  2839,  2003,  5410,  1998,  6881,  2080,\n",
      "         1010,  2021,  2031,  1000, 17936,  6767,  7054,  3401,  1000,  1012,\n",
      "         2111,  2066,  2000, 12826,  1010,  2000,  3648,  1010,  2000, 16157,\n",
      "         1012,  2129,  2055,  2074,  9107,  1029,  6057,  2518,  2205,  1010,\n",
      "         2111,  3015,  3481,  3771,  3504,  2137,  2021,  1010,  2006,  1996,\n",
      "         2060,  2192,  1010,  9177,  2027,  9544,  2137,  2186,  1006,   999,\n",
      "          999,   999,  1007,  1012,  2672,  2009,  1005,  1055,  1996,  2653,\n",
      "         1010,  2030,  1996,  4382,  1010,  2021,  1045,  2228,  2023,  2186,\n",
      "         2003,  2062,  2394,  2084,  2137,  1012,  2011,  1996,  2126,  1010,\n",
      "         1996,  5889,  2024,  2428,  2204,  1998,  6057,  1012,  1996,  3772,\n",
      "         2003,  2025, 23105,  2012,  2035,  1012,  1012,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]),\n",
      " 'label': tensor(1)}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "2e6b2636-af9c-40ec-98a3-22c60075da3d",
   "metadata": {},
   "source": [
    "You will see that the datapoint has an `attention_mask`, `input_ids`, and `label`. The `attention_mask` and `input_ids` are new and were added by `tokenize_fn`.\n",
    "\n",
    "The `input_ids` are the padded sequences of word-piece ids, while the `attention_mask` identifies which parts of the `input_ids` are padding and so should be ignored by the attention layer. \n",
    "\n",
    "Note that the first id in the tensor of `input_ids` is `101`, which represents the `[CLS]` token, while the last non-zero id is `102` which is the `[SEP]` token. If you want to explore a bit further the mapping from tokens to ids can be accessed through the dictionary `tok.vocab`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8898cd26-cfbe-4b42-b41f-34e434df7e3e",
   "metadata": {},
   "source": [
    "### Validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76904945-5d93-4640-9798-ce38caa091c4",
   "metadata": {},
   "source": [
    "We can now create a validation dataset from the `test` data in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ccc1d964-01f7-4bb5-9615-ec5ecfa6a051",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:34:46.152629Z",
     "start_time": "2024-09-28T01:34:39.991573Z"
    }
   },
   "source": [
    "shuffle_val_data = data['test'].shuffle(seed=42)\n",
    "tokenized_val_data = shuffle_val_data.map(tokenize_fn, batched=True)\n",
    "small_val_data = tokenized_val_data.select(range(1000))\n",
    "small_val_data = small_val_data.with_format(\"torch\", columns=[\"label\", \"input_ids\", \"attention_mask\"])"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a30b49359d21412bbb9f938e55e5d4d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7da0e4ac-3015-45e5-81e0-f74c8ce4e932",
   "metadata": {},
   "source": [
    "## Model\n",
    "Defining and downloading the pre-trained transformer is straightforward but be aware that the download is ~270M:"
   ]
  },
  {
   "cell_type": "code",
   "id": "490bc3f8-952c-4ba5-a86f-8eafae50bfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:35:21.458148Z",
     "start_time": "2024-09-28T01:34:59.680755Z"
    }
   },
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90284993f27c4d4aad2bf3fe63d9bba7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "a8b9d737-b4c1-43d4-a958-36f1d90e8773",
   "metadata": {},
   "source": [
    "The above specifies downloads and sets-up a pre-trained `DistilBert` model and configures it for the sequence classification task. \n",
    "\n",
    "The download is specifically the [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) pre-trained model, which only considers lowercase characters (there is a cased version available also `distilbert-base-cased`). \n",
    "\n",
    "Under the hood, the huggingface sequence classifier model will feed the output of the transformer (the output value at the position of the `[CLS]` token) to a new Linear layer to get logits (un-normalised scores for each of the classes). We need to provide the number of classes so the linear layers weight matrix can be properly specified. The way to do this is with the `num_labels` argument. As there are only two classes in our task (positive and negative) we specify `2` as the number of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390874e5-18de-446b-8f94-2ca7b17aefab",
   "metadata": {},
   "source": [
    "The loaded `DistilBert` model is in the variable `model` which is also a PyTorch module with the normal set of PyTorch methods, such as `forward`, `parameters`, and `to`.\n",
    "\n",
    "We first move the model to the desired device such as the GPU (if available) using `model.to(device)`."
   ]
  },
  {
   "cell_type": "code",
   "id": "d3eb6c6f-861f-46c5-a411-99c1c7857e2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:35:33.171549Z",
     "start_time": "2024-09-28T01:35:32.932538Z"
    }
   },
   "source": [
    "model.to(device)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "d25aa483-7ac4-4b80-abc9-5705a8d39f9c",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da3ff6-443c-4569-9ab3-cc625a15101c",
   "metadata": {},
   "source": [
    "We initialize an `AdamW` optimiser, and then start the main training loop. Which consists of a model forward pass, followed by a backward pass and optimiser step. \n",
    "\n",
    "Note that the model takes a number of arguments:\n",
    "- `labels` (optional) which are the ground truth labels (for calculating the loss function when training),\n",
    "- `input_ids` which are the padded sequences of word-pieces, and\n",
    "- `attention_mask` which is an binary mask indicating which parts of the inputs are actual tokens and which are padding tokens (we do not want the transformer to pay attention to any padding tokens);\n",
    "\n",
    "and returns a [SequenceClassifierOutput](https://huggingface.co/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput) object."
   ]
  },
  {
   "cell_type": "code",
   "id": "6c6efe71-e851-424f-b1dd-aaa4bae07584",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T01:36:46.775900Z",
     "start_time": "2024-09-28T01:36:46.765764Z"
    }
   },
   "source": [
    "def finetune_model(model, dataset_train, dataset_val=None, eval_fn=None, batch_size=8, n_epochs=2, learning_rate=1e-5):\n",
    "    model.train(True)\n",
    "    \n",
    "    # create a pytorch data loaders to make it easy to iterate over batches of training data\n",
    "    # see https://pytorch.org/docs/stable/data.html\n",
    "    dataloader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    n_batches = (len(dataset_train) - 1) // batch_size + 1\n",
    "    \n",
    "    # get the AdamW optimizer\n",
    "    optimiser = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # run the training loop\n",
    "    print(f'{\"Epoch\":>10} {\"Batch\":>20} {\"Loss\":>10}')\n",
    "    for epoch in range(n_epochs):\n",
    "        for (b, batch) in enumerate(tqdm(dataloader)):\n",
    "    \n",
    "            # run the transformer forwards\n",
    "            outputs = model(\n",
    "                labels = batch[\"label\"].to(device),\n",
    "                input_ids = batch[\"input_ids\"].to(device),\n",
    "                attention_mask = batch[\"attention_mask\"].to(device),\n",
    "            )\n",
    "    \n",
    "            # get the classification loss\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # backpropagate then apply the optimiser\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "    \n",
    "            # print the loss\n",
    "            if (b+1) % 5 == 0:\n",
    "                print(f'{epoch+1:>10} {f\"{b+1} / {n_batches}\":>20} {format(loss.cpu().item(), \".3f\"):>10}')\n",
    "\n",
    "        # evaluate the model on validation data\n",
    "        if (dataset_val is not None) and (eval_fn is not None):\n",
    "            print('Evaluating ...')\n",
    "            f1 = eval_fn(model, dataset_val)\n",
    "            print(f'Epoch {epoch+1}: F1 score (validation) = {format(f1, \".3f\")}') \n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "8d99068f-83bc-4b57-928b-71ea51bef2a5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Implement a `evaluate_model` function which returns the F1 score of the model on the validation data. This is going to be similar to the evaluation functions you have seen in previous labs/assignments and also reasonably similar to the training loop provided above."
   ]
  },
  {
   "cell_type": "code",
   "id": "e0e2ed4e-5c36-42ec-8eb5-a365520b0fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T11:43:56.153116Z",
     "start_time": "2024-09-29T11:43:56.087149Z"
    }
   },
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset_val, batch_size=8):\n",
    "    # TODO: Implement this function which returns the F1 score of the model on the validation data.\n",
    "    dataloader = DataLoader(dataset_val, batch_size=batch_size)\n",
    "    true_labels = []\n",
    "    predictions = []\n",
    "\n",
    "    model.eval()  # \n",
    "    with torch.no_grad():  # \n",
    "        for batch in dataloader:\n",
    "            inputs = {\n",
    "                'input_ids': batch['input_ids'].to(device),\n",
    "                'attention_mask': batch['attention_mask'].to(device)\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            pred_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            predictions.extend(pred_labels.cpu().numpy())\n",
    "            true_labels.extend(batch['label'].numpy())\n",
    "\n",
    "    # F1\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "    \n",
    "    print(f'Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}')\n",
    "    return f1"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "2932797f-aa30-4f7b-9f2c-07bd491bf142",
   "metadata": {},
   "source": [
    "Fine tune the model and evaluate it on the validation set after each training epoch (this may take ~30 minutes if using CPU)."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d529e89-e968-4042-911f-932afff288c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-29T11:45:10.500215Z",
     "start_time": "2024-09-29T11:43:57.943673Z"
    }
   },
   "source": [
    "model = finetune_model(model, small_train_data, small_val_data, eval_fn=evaluate_model, n_epochs=3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Epoch                Batch       Loss\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c726da8a7324a7fa28f579422a7468e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1              5 / 125      0.687\n",
      "         1             10 / 125      0.672\n",
      "         1             15 / 125      0.676\n",
      "         1             20 / 125      0.693\n",
      "         1             25 / 125      0.698\n",
      "         1             30 / 125      0.688\n",
      "         1             35 / 125      0.663\n",
      "         1             40 / 125      0.662\n",
      "         1             45 / 125      0.647\n",
      "         1             50 / 125      0.565\n",
      "         1             55 / 125      0.500\n",
      "         1             60 / 125      0.439\n",
      "         1             65 / 125      0.631\n",
      "         1             70 / 125      0.541\n",
      "         1             75 / 125      0.651\n",
      "         1             80 / 125      0.508\n",
      "         1             85 / 125      0.444\n",
      "         1             90 / 125      0.257\n",
      "         1             95 / 125      0.585\n",
      "         1            100 / 125      0.206\n",
      "         1            105 / 125      0.307\n",
      "         1            110 / 125      0.522\n",
      "         1            115 / 125      0.450\n",
      "         1            120 / 125      0.123\n",
      "         1            125 / 125      0.447\n",
      "Evaluating ...\n",
      "Precision: 0.828, Recall: 0.772, F1: 0.758\n",
      "Epoch 1: F1 score (validation) = 0.758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9cfeba8f01148a3a758774c7e42e9a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2              5 / 125      0.388\n",
      "         2             10 / 125      0.454\n",
      "         2             15 / 125      0.261\n",
      "         2             20 / 125      0.083\n",
      "         2             25 / 125      0.372\n",
      "         2             30 / 125      0.341\n",
      "         2             35 / 125      0.153\n",
      "         2             40 / 125      0.137\n",
      "         2             45 / 125      0.106\n",
      "         2             50 / 125      0.068\n",
      "         2             55 / 125      0.371\n",
      "         2             60 / 125      0.304\n",
      "         2             65 / 125      0.174\n",
      "         2             70 / 125      0.196\n",
      "         2             75 / 125      0.109\n",
      "         2             80 / 125      0.259\n",
      "         2             85 / 125      0.364\n",
      "         2             90 / 125      0.533\n",
      "         2             95 / 125      0.269\n",
      "         2            100 / 125      0.081\n",
      "         2            105 / 125      0.465\n",
      "         2            110 / 125      0.116\n",
      "         2            115 / 125      0.103\n",
      "         2            120 / 125      0.136\n",
      "         2            125 / 125      0.055\n",
      "Evaluating ...\n",
      "Precision: 0.862, Recall: 0.858, F1: 0.857\n",
      "Epoch 2: F1 score (validation) = 0.857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "340eaed8c99e42dab853845d6e3ad894"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         3              5 / 125      0.040\n",
      "         3             10 / 125      0.558\n",
      "         3             15 / 125      0.035\n",
      "         3             20 / 125      0.036\n",
      "         3             25 / 125      0.589\n",
      "         3             30 / 125      0.036\n",
      "         3             35 / 125      0.041\n",
      "         3             40 / 125      0.455\n",
      "         3             45 / 125      0.028\n",
      "         3             50 / 125      0.030\n",
      "         3             55 / 125      0.051\n",
      "         3             60 / 125      0.035\n",
      "         3             65 / 125      0.045\n",
      "         3             70 / 125      0.047\n",
      "         3             75 / 125      0.052\n",
      "         3             80 / 125      0.186\n",
      "         3             85 / 125      0.022\n",
      "         3             90 / 125      0.022\n",
      "         3             95 / 125      0.025\n",
      "         3            100 / 125      0.032\n",
      "         3            105 / 125      0.024\n",
      "         3            110 / 125      0.030\n",
      "         3            115 / 125      0.046\n",
      "         3            120 / 125      0.459\n",
      "         3            125 / 125      0.218\n",
      "Evaluating ...\n",
      "Precision: 0.870, Recall: 0.870, F1: 0.869\n",
      "Epoch 3: F1 score (validation) = 0.869\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
